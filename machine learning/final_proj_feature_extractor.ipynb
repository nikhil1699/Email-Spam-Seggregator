{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6142a19e",
   "metadata": {},
   "source": [
    "# Feature Extraction: This file is used for parsing emails and extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ea0a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "541f9155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/NSD/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/NSD/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094344bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59cf6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file and write names of the colums run this only once\n",
    "with open('spam_x.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"file_name\", \"volume\", \"subject\", \"body\",\"ham/spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8e7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "NEWLINE = '\\n'\n",
    "CLEAN = re.compile(r'<[^>]+>')\n",
    "SW_NLTK = set(stopwords.words('english'))\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "    ('data/beck-s',      HAM),\n",
    "    ('data/farmer-d',    HAM),\n",
    "    ('data/kaminski-v',  HAM),\n",
    "    ('data/kitchen-l',   HAM),\n",
    "    ('data/lokay-m',     HAM),\n",
    "    ('data/williams-w3', HAM),\n",
    "    ('data/BG',          SPAM),\n",
    "    ('data/GP',          SPAM),\n",
    "    ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "# extracts and preprocess subject from the header\n",
    "def extract_subject(header):\n",
    "    lines = header.splitlines()\n",
    "    for line in lines:\n",
    "        if \"subject:\" in line.lower():\n",
    "            subject = line.lower().replace('subject: ', '')\n",
    "            subject = preprocess_body(subject)\n",
    "            return subject\n",
    "    \n",
    "def preprocess_body(body):\n",
    "    # remove extra white spaces\n",
    "    body = \" \".join(body.split())\n",
    "    \n",
    "    # remove numbers\n",
    "    body = ''.join([i for i in body if not i.isdigit()])\n",
    "    \n",
    "    # remove urls\n",
    "    body = re.sub(r'http\\S+', '', body)\n",
    "    \n",
    "    # remove all HTML tags\n",
    "    body = re.sub(CLEAN, '', body)\n",
    "    \n",
    "    # to lowercase\n",
    "    body = body.lower()\n",
    "    \n",
    "    # apply stop words removal and stemming\n",
    "    processed_body = \"\"\n",
    "    porter = PorterStemmer()\n",
    "    word_tokens = word_tokenize(body)\n",
    "    for word in word_tokens:\n",
    "        if word not in SW_NLTK:\n",
    "            stemmed = porter.stem(word.lower())\n",
    "            processed_body += stemmed + \" \"\n",
    "\n",
    "    #remove punctuation\n",
    "    processed_body = processed_body.translate(str.maketrans('', '', string.punctuation))\n",
    "    return processed_body\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        # crawl every path\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, header, body = False, [], []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    # get the header and body\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            body.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                        else:\n",
    "                            header.append(line)\n",
    "                    f.close()\n",
    "                    header = NEWLINE.join(header)\n",
    "                    body = NEWLINE.join(body)\n",
    "                    subject = extract_subject(header)  # get the subject from header\n",
    "                    yield file_path, body, subject\n",
    "\n",
    "\n",
    "def load_file(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, body, subject) in enumerate(read_files(path)):\n",
    "        volume = len(body)\n",
    "        body = preprocess_body(body)\n",
    "        # write features to csv\n",
    "        with open('spam_x.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([file_name, volume, subject, body, classification])\n",
    "            \n",
    "\n",
    "def load_data():\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        load_file(path, classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ee8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
